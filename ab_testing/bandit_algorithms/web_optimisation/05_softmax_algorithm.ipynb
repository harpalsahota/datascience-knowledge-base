{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Softmax Algorithm\n",
    "\n",
    "The problem with the epsilon-Greedy algorithm: it explores options completely at random without any concern about their merits. For example, in one scenario (call it Scenario A), you might have two arms, one of which rewards you 10% of the time and the other rewards you 13% of the time. In Scenario B, the two arms might reward you 10% of the time and 99% of the time. In both of these scenarios, the probability that the epsilon-Greedy algorithm explores the worse arm is exactly the same (it’s `epsilon` / 2), despite the inferior arm in Scenario B being, in relative terms, much worse than the inferior arm in Scenario A.\n",
    "\n",
    "This is a problem for several reasons:\n",
    "- If the difference in reward rates between two arms is small, you’ll need to explore a lot more often than 10% of the time to correctly determine which of the two options is actually better.\n",
    "- In contrast, if the difference is large, you need to explore a lot less than 10% of the time to correctly estimate the better of the two options. For that reason, you’ll end up losing a lot of reward by exploring an unambiguously inferior option in this case. When we first described the epsilon-Greedy algorithm, we said that we wouldn’t set epsilon = 1.0 precisely so that we wouldn’t waste time on inferior options, but, if the difference between two arms is large enough, we end up wasting time on inferior options simply because the epsilon-Greedy algorithm always explores completely at random.\n",
    "\n",
    "Putting these two points together, it seems clear that there’s a qualitative property missing from the epsilon-Greedy algorithm. We need to make our bandit algorithm cares about the known differences between the estimated values of the arms when our algorithm decides which arm to explore. We need structured exploration rather than the haphazard exploration that the epsilon-Greedy algorithm provides.\n",
    "\n",
    "The first algorithm we’ll describe that takes this structural information into account is called the Softmax algorithm. The Softmax algorithm tries to cope with arms differing in estimated value by explicitly incorporating information about the reward rates of the available arms into its method for choosing which arm to select when it explores.\n",
    "\n",
    "You can get an initial intuition for how the Softmax algorithm handles this problem by imagining that you choose each arm in proportion to its estimated value. Suppose that you have two arms, A and B. Now imagine that, based on your past experiences, these two arms have had two different rates of success: `rA` and `rB`. With those assumptions, the most naive possible implementation of a Softmax-like algorithm would have you choose Arm A with probability `rA / (rA + rB)` and Arm B with probability `rB / (rA + rB)`\n",
    "\n",
    "In code this would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def categorical_draw(probs):\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for i in range(len(probs)):\n",
    "        prob = probs[i]\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z:\n",
    "            return i\n",
    "    return len(probs) - 1\n",
    "\n",
    "def select_arm(self):\n",
    "    z = sum(self.values)\n",
    "    probs = [v / z for v in self.values]\n",
    "    return categorical_draw(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, this very naive algorithm isn’t something people actually use. To reconstruct the algorithm people actually use, we need to make two changes to it.\n",
    "\n",
    "First, we will calculate a different scale for reward rates by exponentiating our estimates of `rA` and `rB`. Using this new scale, we will choose Arm A with probability `exp(rA) / (exp(rA) + exp(rB))` and Arm B with probability `exp(rB) / (exp(rA) + exp(rB))`. This naive exponential rescaling has the virtue of not behaving strangely if you someone used negative numbers as rates of success, since the call to exp will turn any negative numbers into positive numbers and insure that the negative numbers in the denominator of these fractions can’t cancel out any positive numbers that may be found in the denominator.\n",
    "\n",
    "More importantly, this exponentiation trick brings us very close to the full Softmax algorithm. In fact, plain exponential rescaling gives us the Softmax algorithm if you hardcoded one of the configurable parameters that the standard Softmax algorithm possesses. This additional parameter is a different sort of scaling factor than the exponentiation we just introduced.\n",
    "\n",
    "This new type of scaling factor is typically called a temperature parameter based on an analogy with physics in which systems at high temperatures tend to behave randomly, while they take on more structure at low temperatures. In fact, the full Softmax algorithm is closely related to a concept called the Boltzmann distribution in physics, which is used to describe how groups of particles behave.\n",
    "\n",
    "We’ll call this new temperature parameter tau. We introduce `tau` to produce the following new algorithm:\n",
    "- At time T, select one of the two arms with probabilities computed as follows:\n",
    "    - `exp(rA / tau) / (exp(rA / tau) + exp(rB / tau))`\n",
    "    - `exp(rB / tau) / (exp(rA / tau) + exp(rB / tau))`\n",
    "    \n",
    "- For whichever arm you picked, update your estimate of the mean using the same update rule we used for the epsilon-Greedy algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Success rate of arm A is x3 less than of B. \n",
    "\"\"\"\n",
    "success_rate_arm_a = 0.2\n",
    "success_rate_arm_b = 0.6\n",
    "temp = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017986209962091573"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prob of selecting Arm A\n",
    "np.exp(success_rate_arm_a / temp) / (np.exp(success_rate_arm_a / temp) + np.exp(success_rate_arm_b / temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9820137900379083"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prob of selecting Arm B\n",
    "np.exp(success_rate_arm_b / temp) / (np.exp(success_rate_arm_a / temp) + np.exp(success_rate_arm_b / temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
