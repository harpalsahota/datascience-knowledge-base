{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification models aim to provide an automated decision such as is this e-mail spam?\n",
    "Most classification models return a probability or propensity of belonging to a class of interest.\n",
    "A cut-off can then be used to convert the propensity score to a decision. It follows the general approach:\n",
    "\n",
    "1. Establish a cutoff probability for the class of interest above which we consider a record as belonging to that class.\n",
    "2. Estimate (with any model) the probability that a record belongs to the class of interest.\n",
    "3. If that probability is above the cutoff probability, assign the new record to the class of interest.\n",
    "\n",
    "The higher the cutoff, the fewer records predicted as that class.\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "The naive Bayes algorithm uses the probability of observing predictor values,\n",
    "given an outcome, to estimate the probability of observing outcome $Y = i$, given\n",
    "a set of predictor values.\n",
    "\n",
    "- **Conditional probability**: The probability of observing some event (say $X = i$) given some other event (say $Y = i$), written as $P(X_i | Y_i)$.\n",
    "- **Posterior probability**: The probability of an outcome after the predictor information has been incorporated (in contrast to the prior probability of outcomes, not taking predictor information into account).\n",
    "\n",
    "To understand Bayesian classification, we can start out by imagining \"nonnaive\"\n",
    "Bayesian classification. For each record to be classified:\n",
    "1. Find all the other records with the same predictor profile (i.e., where the predictor values are the same).\n",
    "2. Determine what classes those records belong to and which class is most prevalent (i.e., probable).\n",
    "3. Assign that class to the new record.\n",
    "\n",
    "The preceding approach amounts to finding all the records in the sample that are\n",
    "exactly like the new record to be classified in the sense that all the predictor\n",
    "values are identical.\n",
    "\n",
    "### Why Exact Bayesian Classification Is Impractical\n",
    "\n",
    "When the number of predictor variables exceeds a handful, many of the records\n",
    "to be classified will be without exact matches e.g. male Hispanic with high income from the US Midwest who voted in the last\n",
    "election, did not vote in the prior election, has three daughters and one son, and\n",
    "is divorced.\n",
    "\n",
    "_Despite its name, naive Bayes is not considered a method of Bayesian statistics. Naive Bayes\n",
    "is a data–driven, empirical method requiring relatively little statistical expertise. The name\n",
    "comes from the Bayes rule–like calculation in forming the predictions—specifically the initial\n",
    "calculation of predictor value probabilities given an outcome, and then the final calculation of\n",
    "outcome probabilities._\n",
    "\n",
    "### The Naive Solution\n",
    "\n",
    "In the naive Bayes solution, we no longer restrict the probability calculation to\n",
    "those records that match the record to be classified. Instead, we use the entire\n",
    "data set. The naive Bayes modification is as follows:\n",
    "\n",
    "1. For a binary response $Y = i$ ($i = 0$ or $1$), estimate the individual conditional probabilities for each predictor $P(X_j|Y=i)$; these are the probabilities that the predictor value is in the record when we observe $Y = i$. This probability is estimated by the proportion of $X_j$ values among the $Y = i$ records in the training set\n",
    "2. Multiply these probabilities by each other, and then by the proportion of records belonging to $Y = i$\n",
    "3. Repeat steps 1 and 2 for all the classes\n",
    "4. Estimate a probability for outcome $i$ by taking the value calculated in step 2 for class $i$ and dividing it by the sum of such values for all classes.\n",
    "5. Assign the record to the class with the highest probability for this set of predictor values.\n",
    "\n",
    "This naive Bayes algorithm can also be stated as an equation for the probability\n",
    "of observing outcome $Y = i$, given a set of predictor values $X_1, ...,X_p$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X_1, X_2, ..., X_p)\n",
    "\\end{equation}\n",
    "\n",
    "The value of $P(X_1, X_2, ..., X_p)$ is a scaling factor to ensure the probability\n",
    "is between 0 and 1 and does not depend on Y:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X_1, X_2, ..., X_p) = P(Y=0)(P(X_1|Y=0)P(X_2|Y=0)...P(X_p|Y=0)) + P(Y=1)(P(X_1|Y=1)P(X_2|Y=1)...P(X_p|Y=1))\n",
    "\\end{equation}\n",
    "\n",
    "**Why is this formula called “naive”? We have made a simplifying assumption\n",
    "that the exact conditional probability of a vector of predictor values, given\n",
    "observing an outcome, is sufficiently well estimated by the product of the\n",
    "individual conditional probabilities $P(X_j|Y=i)$. In other words, in\n",
    "estimating $P(X_j|Y=i)$ instead of $P(X_1, X_2, ..., X_p | Y=i)$, we are assuming $X_j$\n",
    "is independent of all the other predictor variables $X_{k for} K\\neq j$**\n",
    "\n",
    "In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)\n",
    "\n",
    "Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.\n",
    "\n",
    "On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
    "\n",
    "#### Note\n",
    "_When a predictor category is absent in the training data, the algorithm assigns zero probability\n",
    "to the outcome variable in new data, rather than simply ignoring this variable and using the\n",
    "information from other variables, as other methods might. This is something to pay attention to\n",
    "when binning continuous variables._\n",
    "\n",
    "### Numeric Predictor Variables\n",
    "\n",
    "From the definition, we see that the Bayesian classifier works only with\n",
    "categorical predictors (e.g., with spam classification, where presence or absence\n",
    "of words, phrases, characters, and so on, lies at the heart of the predictive task).\n",
    "To apply naive Bayes to numerical predictors, one of two approaches must be\n",
    "taken:\n",
    "- Bin and convert the numerical predictors to categorical predictors and apply the algorithm of the previous section.\n",
    "- Use a probability model—for example, the normal distribution to estimate the conditional probability $P(X_j|Y=i)$\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Naive Bayes works with categorical (factor) predictors and outcomes.\n",
    "- It asks, \"Within each outcome category, which predictor categories are most probable?\"\n",
    "- That information is then inverted to estimate probabilities of outcome categories, given predictor values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminant Analysis\n",
    "\n",
    "Discriminant analysis is the earliest statistical classifier; it was introduced by R.\n",
    "A. Fisher in 1936\n",
    "\n",
    "- **Covariance**: A measure of the extent to which one variable varies in concert with another (i.e., similar magnitude and direction).\n",
    "- **Discriminant function**: The function that, when applied to the predictor variables, maximizes the separation of the classes.\n",
    "- **Discriminant weights**: The scores that result from the application of the discriminant function, and are used to estimate probabilities of belonging to one class or another.\n",
    "\n",
    "While discriminant analysis encompasses several techniques, the most commonly used is linear discriminant analysis, or LDA. LDA is now less widely used with the advent of more\n",
    "sophisticated techniques, such as tree models and logistic regression.\n",
    "\n",
    "However, you may still encounter LDA in some applications and it has links to\n",
    "other more widely used methods (such as principal components analysis). In addition, discriminant analysis can\n",
    "provide a measure of predictor importance, and it is used as a computationally\n",
    "efficient method of feature selection.\n",
    "\n",
    "#### Note\n",
    "Linear discriminant analysis should not be confused with Latent Dirichlet Allocation, also\n",
    "referred to as LDA. Latent Dirichlet Allocation is used in text and natural language processing\n",
    "and is unrelated to linear discriminant analysis.\n",
    "\n",
    "### Covariance Matrix\n",
    "\n",
    "covariance is a measure of the joint variability of two random variables. If the greater values of one variable mainly correspond with the greater values of the other variable, and the same holds for the lesser values (that is, the variables tend to show similar behavior), the covariance is positive. In the opposite case, when the greater values of one variable mainly correspond to the lesser values of the other, (that is, the variables tend to show opposite behavior), the covariance is negative. The sign of the covariance therefore shows the tendency in the linear relationship between the variables. The magnitude of the covariance is not easy to interpret because it is not normalized and hence depends on the magnitudes of the variables\n",
    "\n",
    "To understand discriminant analysis, it is first necessary to introduce the concept\n",
    "of covariance between two or more variables. The covariance measures the\n",
    "relationship between two variables $x$ and $z$. Denote the mean for each variable\n",
    "by $\\overline{x}$ and $\\overline{z}$. The covariance $S_{x,z}$ between $x$ and $z$ is given by:\n",
    "\\begin{equation}\n",
    "S_{x,z} = \\frac{\\sum_{i=1}^{n}(x-\\overline{x})(z-\\overline{z})}{n-1}\n",
    "\\end{equation}\n",
    "where n is the number of records.\n",
    "\n",
    "As with the correlation coefficient, positive values indicate a\n",
    "positive relationship and negative values indicate a negative relationship.\n",
    "Correlation, however, is constrained to be between –1 and 1, whereas covariance\n",
    "is on the same scale as the variables $x$ and $z$. The covariance matrix $\\sum$ for $x$\n",
    "and $z$ consists of the individual variable variances, $S^2_x$ and $S^2_y$, on the diagonal\n",
    "(where row and column are the same variable) and the covariances between\n",
    "variable pairs on the off-diagonals.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\sum} = \\begin{bmatrix}\n",
    "S^2_x & S_{x,z} \\\\\n",
    "S_{x,z} & S^2_z \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Fisher’s Linear Discriminant\n",
    "\n",
    "For simplicity, we focus on a classification problem in which we want to predict\n",
    "a binary outcome y using just two continuous numeric variables $(x,z)$.\n",
    "Technically, discriminant analysis assumes the predictor variables are normally\n",
    "distributed continuous variables, but, in practice, the method works well even for\n",
    "nonextreme departures from normality, and for binary predictors. Fisher’s linear\n",
    "discriminant distinguishes variation between groups, on the one hand, from\n",
    "variation within groups on the other. Specifically, seeking to divide the records\n",
    "into two groups, LDA focuses on maximizing the \"between\" sum of squares $SS_{between}$\n",
    "(measuring the variation between the two groups) relative to the\n",
    "\"within\" sum of squares $SS_{within}$ (measuring the within-group variation). In this\n",
    "case, the two groups correspond to the records $(x_0, z_0)$ for which $y = 0$ and the\n",
    "records $(x_1, z_1)$ for which $y = 1$. The method finds the linear combination $w_{x}x+w_{z}z$\n",
    "that maximizes that sum of squares ratio.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{SS_{between}}{SS_{within}}\n",
    "\\end{equation}\n",
    "\n",
    "The between sum of squares is the squared distance between the two group\n",
    "means, and the within sum of squares is the spread around the means within each\n",
    "group, weighted by the covariance matrix. Intuitively, by maximizing the\n",
    "between sum of squares and minimizing the within sum of squares, this method\n",
    "yields the greatest separation between the two groups.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Discriminant analysis works with continuous or categorical predictors, as well as categorical outcomes.\n",
    "- Using the covariance matrix, it calculates a linear discriminant function, which is used to distinguish records belonging to one class from those belonging to another.\n",
    "- This function is applied to the records to derive weights, or scores, for each record (one weight for each possible class) that determines its estimated class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
