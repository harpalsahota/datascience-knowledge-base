{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Machine Learning\n",
    "\n",
    "## Machine Learning vs. Statistics\n",
    "\n",
    "In the context of predictive modeling, what is the difference between machine learning and\n",
    "statistics? There is not a bright line dividing the two disciplines. Machine learning tends to be\n",
    "more focused on developing efficient algorithms that scale to large data in order to optimize\n",
    "the predictive model. Statistics generally pays more attention to the probabilistic theory and\n",
    "underlying structure of the model. Bagging, and the random forest (see “Bagging and the\n",
    "Random Forest”), grew up firmly in the statistics camp. Boosting (see “Boosting”), on the\n",
    "other hand, has been developed in both disciplines but receives more attention on the machine\n",
    "learning side of the divide. Regardless of the history, the promise of boosting ensures that it\n",
    "will thrive as a technique in both statistics and machine learning.\n",
    "\n",
    "## K-Nearest Neighbors\n",
    "\n",
    "The idea behind K-Nearest Neighbors (KNN) is very simple. For each record to\n",
    "be classified or predicted:\n",
    "1. Find K records that have similar features (i.e., similar predictor values).\n",
    "2. For classification: Find out what the majority class is among those similar records, and assign that class to the new record.\n",
    "3. For prediction (also called KNN regression): Find the average among those similar records, and predict that average for the new record.\n",
    "\n",
    "- **Neighbor**: A record that has similar predictor values to another record.\n",
    "- **Distance metrics**: Measures that sum up in a single number how far one record is from another.\n",
    "- **Standardization**: Subtract the mean and divide by the standard deviation.\n",
    "    - _Synonym_: Normalization\n",
    "- **Z-score**: The value that results after standardization\n",
    "- **K**: The number of neighbors considered in the nearest neighbor calculation\n",
    "\n",
    "The prediction results depend on how the features are scaled, how\n",
    "similarity is measured, and how big K is set. Also, all predictors must be in\n",
    "numeric form\n",
    "\n",
    "While the output of KNN for classification is typically a binary decision, such as default or\n",
    "paid off in the loan data, KNN routines usually offer the opportunity to output a probability\n",
    "(propensity) between 0 and 1. The probability is based on the fraction of one class in the $K$\n",
    "nearest neighbors. In the preceding example, this probability of default would have been\n",
    "estimated at $\\frac{14}{20}$ or 0.7. Using a probability score lets you use classification rules other than\n",
    "simple majority votes (probability of 0.5). This is especially important in problems with\n",
    "imbalanced classes; see “Strategies for Imbalanced Data”. For example, if the goal is to\n",
    "identify members of a rare class, the cutoff would typically be set below 50%. One common\n",
    "approach is to set the cutoff at the probability of the rare event.\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "Similarity (nearness) is determined using a distance metric, which is a function\n",
    "that measures how far two records $(x_1, x_2, \\cdots, x_p)$ and $(u_1, u_2, \\cdots, u_p)$ are from one\n",
    "another. The most popular distance metric between two vectors is Euclidean\n",
    "distance. To measure the Euclidean distance between two vectors, subtract one\n",
    "from the other, square the differences, sum them, and take the square root:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{(x_1 - u_1)^2 +  (x_2 - u_2)^2 + \\cdots + (x_p - u_p)^2}\n",
    "\\end{equation}\n",
    "\n",
    "Another common distance metric for numeric data is Manhattan distance:\n",
    "\n",
    "\\begin{equation}\n",
    "|x_1 - u_1| +  |x_2 - u_2| + \\cdots + |x_p - u_p|\n",
    "\\end{equation}\n",
    "\n",
    "Euclidean distance corresponds to the straight-line distance between two points. \n",
    "Manhattan distance is the distance between two points\n",
    "traversed in a single direction at a time (e.g., traveling along rectangular city\n",
    "blocks). For this reason, Manhattan distance is a useful approximation if\n",
    "similarity is defined as point-to-point travel time.\n",
    "\n",
    "In measuring distance between two vectors, variables (features) that are\n",
    "measured with comparatively large scale will dominate the measure. For\n",
    "example, for the loan data, the distance would be almost solely a function of the\n",
    "income and loan amount variables, which are measured in tens or hundreds of\n",
    "thousands. Ratio variables would count for practically nothing in comparison.\n",
    "We address this problem by standardizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
