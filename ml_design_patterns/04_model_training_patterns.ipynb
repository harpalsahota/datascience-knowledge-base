{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Patterns\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- On large datasets stochastic gradient descent (SGD) is applied to mini-batches\n",
    "- This is called stochastic SGD and extensions of SGD e.g. Adam, Adamgrad etc... are the de facto optimiser used in modern-day machine learning frameworks\n",
    "\n",
    "- SGD requires training to take place iteratively on small batches therefore training happens in a loop\n",
    "- SGD finds a minimum, but not a closed-form solution, and so we have to detect whether the model convergence has happened\n",
    "- As a result, the error (called the loss) on the training dataset has to be monitored.\n",
    "- Overfitting can happen if the model complexity is higher than can be affored by the size and coverage of the dataset\n",
    "- It is difficult to know if the compexity is too high until we actually train the model on the dataset\n",
    "- Therefore, evaluation needs to be done within the training loop and error metrics on a witheld split of the training data (validation set).\n",
    "- Because training and validation datasets have been used i the training loop it is necessary to withhold yet another split of the training dataset called the test set.\n",
    "- Metrics are reported on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 11: Useful Overfitting\n",
    "\n",
    "- Want to intentionally overfit on the training dataset\n",
    "- Perform training without regularisation, dropout, validation dataset or early stopping\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Goal of a ML model is to generalise well to make good predictions on unseen data.\n",
    "- If the model overfits then the ability to generalise suffers and so do future predictions\n",
    "\n",
    "- For example, imagine we have system to model the physical enviroment\n",
    "- The model carries out iterative, numerical calculations to calculate the precise state of the system\n",
    "- Suppose all observations have a finite number of possibilites e.g. temperature is limited to 60 - 80 degrees celcius in increments of 0.01.\n",
    "- We can then create a training dataset for the ML system consisting of the complete input space and calculate lavels using the physical model\n",
    "- Splitting the training dataset would be counterproductive because we would then be expecting the model to learn parts of the input space it will not have seen in the training dataset.\n",
    "\n",
    "### Solutions\n",
    "\n",
    "- In the above scenario, there is no \"unseen\" data that needs to be generalised to, since all possible inputs have been tabulated\n",
    "- If all possible inputs to a model can be tabulated there is no such thing as overfitting\n",
    "\n",
    "- Typically, overfitting of the training dataset in this way causes the model to give misguided predictions on new, unseen datapoints\n",
    "- The difference here is that we know in advance there won't be unseen data\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "- If all possible inputs can be tabulated, then an overfit model will make the same predictions as the \"true\" model if all possible inputs are trained for, so overfitting is not a concern\n",
    "\n",
    "- Overfitting is useful when:\n",
    "    - There is no noise, so the labels are accurate for all instances\n",
    "    - You have the complete dataset as your disposal (you have all the examples there are). In this case, overfitting becomes interpolating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 12: Checkpoints\n",
    "\n",
    "- With checkpoints we store the full state of the model periodically so that we have partially trained models available\n",
    "- These models can serve as the final model in the case of the final model or as a starting point for continued training\n",
    "\n",
    "### Problem\n",
    "\n",
    "- More complex model, more data is needed to train effectively\n",
    "- More complex models tend to have more tunable parameters\n",
    "- As model size increases the longer it takes to fit on one batch of data \n",
    "- As data increases the number of batches increases\n",
    "- In terms of computational complexity this is a double whammy\n",
    "\n",
    "- When training for a long time the chances of machine failure increases.\n",
    "- If there is a problem we would like to resume from an intermediate point\n",
    "\n",
    "### Solution\n",
    "\n",
    "- At the end of every epoch save the model state\n",
    "- If a machine failure occures we can resume from the saved state and restart\n",
    "- Make sure the full model state is saved not the just the model\n",
    "- Once training is complete and exported it is usually only the information required to make a prediciton is saved\n",
    "\n",
    "- Good to save information about the training loop as well e.g.\n",
    "    - Learning rate in a learning rate scheduler\n",
    "    - Batch number\n",
    "- Saving a full model state so that is can be resumed is called checkpointing\n",
    "- Model states changes with every batch but there is much overhead to save at every batch so do it at the end of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example to save model state in pytorch\n",
    "\n",
    "```\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_stat_dict': model.state_dict(),\n",
    "    'optimiser_state_dict': optimiser.state_dict(),\n",
    "    'loss': loss,\n",
    "    ...\n",
    "}, PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it Works\n",
    "\n",
    "- Most ML frameworks can result from a saved checkpoint\n",
    "- Checkpoints are designed for mainly for resilience, their availability however opens other use cases\n",
    "- Partially trained models are usually more generalisable that models created later iterations \n",
    "    - [See here](https://playground.tensorflow.org/)\n",
    "    \n",
    "### Trade-Offs and Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
