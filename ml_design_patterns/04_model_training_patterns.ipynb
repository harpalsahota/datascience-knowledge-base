{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Patterns\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- On large datasets stochastic gradient descent (SGD) is applied to mini-batches\n",
    "- This is called stochastic SGD and extensions of SGD e.g. Adam, Adamgrad etc... are the de facto optimiser used in modern-day machine learning frameworks\n",
    "\n",
    "- SGD requires training to take place iteratively on small batches therefore training happens in a loop\n",
    "- SGD finds a minimum, but not a closed-form solution, and so we have to detect whether the model convergence has happened\n",
    "- As a result, the error (called the loss) on the training dataset has to be monitored.\n",
    "- Overfitting can happen if the model complexity is higher than can be affored by the size and coverage of the dataset\n",
    "- It is difficult to know if the compexity is too high until we actually train the model on the dataset\n",
    "- Therefore, evaluation needs to be done within the training loop and error metrics on a witheld split of the training data (validation set).\n",
    "- Because training and validation datasets have been used i the training loop it is necessary to withhold yet another split of the training dataset called the test set.\n",
    "- Metrics are reported on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 11: Useful Overfitting\n",
    "\n",
    "- Want to intentionally overfit on the training dataset\n",
    "- Perform training without regularisation, dropout, validation dataset or early stopping\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Goal of a ML model is to generalise well to make good predictions on unseen data.\n",
    "- If the model overfits then the ability to generalise suffers and so do future predictions\n",
    "\n",
    "- For example, imagine we have system to model the physical enviroment\n",
    "- The model carries out iterative, numerical calculations to calculate the precise state of the system\n",
    "- Suppose all observations have a finite number of possibilites e.g. temperature is limited to 60 - 80 degrees celcius in increments of 0.01.\n",
    "- We can then create a training dataset for the ML system consisting of the complete input space and calculate lavels using the physical model\n",
    "- Splitting the training dataset would be counterproductive because we would then be expecting the model to learn parts of the input space it will not have seen in the training dataset.\n",
    "\n",
    "### Solutions\n",
    "\n",
    "- In the above scenario, there is no \"unseen\" data that needs to be generalised to, since all possible inputs have been tabulated\n",
    "- If all possible inputs to a model can be tabulated there is no such thing as overfitting\n",
    "\n",
    "- Typically, overfitting of the training dataset in this way causes the model to give misguided predictions on new, unseen datapoints\n",
    "- The difference here is that we know in advance there won't be unseen data\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "- If all possible inputs can be tabulated, then an overfit model will make the same predictions as the \"true\" model if all possible inputs are trained for, so overfitting is not a concern\n",
    "\n",
    "- Overfitting is useful when:\n",
    "    - There is no noise, so the labels are accurate for all instances\n",
    "    - You have the complete dataset as your disposal (you have all the examples there are). In this case, overfitting becomes interpolating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 12: Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
