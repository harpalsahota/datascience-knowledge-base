{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Patterns\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "- On large datasets stochastic gradient descent (SGD) is applied to mini-batches\n",
    "- This is called stochastic SGD and extensions of SGD e.g. Adam, Adamgrad etc... are the de facto optimiser used in modern-day machine learning frameworks\n",
    "\n",
    "- SGD requires training to take place iteratively on small batches therefore training happens in a loop\n",
    "- SGD finds a minimum, but not a closed-form solution, and so we have to detect whether the model convergence has happened\n",
    "- As a result, the error (called the loss) on the training dataset has to be monitored.\n",
    "- Overfitting can happen if the model complexity is higher than can be affored by the size and coverage of the dataset\n",
    "- It is difficult to know if the compexity is too high until we actually train the model on the dataset\n",
    "- Therefore, evaluation needs to be done within the training loop and error metrics on a witheld split of the training data (validation set).\n",
    "- Because training and validation datasets have been used i the training loop it is necessary to withhold yet another split of the training dataset called the test set.\n",
    "- Metrics are reported on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 11: Useful Overfitting\n",
    "\n",
    "- Want to intentionally overfit on the training dataset\n",
    "- Perform training without regularisation, dropout, validation dataset or early stopping\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Goal of a ML model is to generalise well to make good predictions on unseen data.\n",
    "- If the model overfits then the ability to generalise suffers and so do future predictions\n",
    "\n",
    "- For example, imagine we have system to model the physical enviroment\n",
    "- The model carries out iterative, numerical calculations to calculate the precise state of the system\n",
    "- Suppose all observations have a finite number of possibilites e.g. temperature is limited to 60 - 80 degrees celcius in increments of 0.01.\n",
    "- We can then create a training dataset for the ML system consisting of the complete input space and calculate lavels using the physical model\n",
    "- Splitting the training dataset would be counterproductive because we would then be expecting the model to learn parts of the input space it will not have seen in the training dataset.\n",
    "\n",
    "### Solutions\n",
    "\n",
    "- In the above scenario, there is no \"unseen\" data that needs to be generalised to, since all possible inputs have been tabulated\n",
    "- If all possible inputs to a model can be tabulated there is no such thing as overfitting\n",
    "\n",
    "- Typically, overfitting of the training dataset in this way causes the model to give misguided predictions on new, unseen datapoints\n",
    "- The difference here is that we know in advance there won't be unseen data\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "- If all possible inputs can be tabulated, then an overfit model will make the same predictions as the \"true\" model if all possible inputs are trained for, so overfitting is not a concern\n",
    "\n",
    "- Overfitting is useful when:\n",
    "    - There is no noise, so the labels are accurate for all instances\n",
    "    - You have the complete dataset as your disposal (you have all the examples there are). In this case, overfitting becomes interpolating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 12: Checkpoints\n",
    "\n",
    "- With checkpoints we store the full state of the model periodically so that we have partially trained models available\n",
    "- These models can serve as the final model in the case of the final model or as a starting point for continued training\n",
    "\n",
    "### Problem\n",
    "\n",
    "- More complex model, more data is needed to train effectively\n",
    "- More complex models tend to have more tunable parameters\n",
    "- As model size increases the longer it takes to fit on one batch of data \n",
    "- As data increases the number of batches increases\n",
    "- In terms of computational complexity this is a double whammy\n",
    "\n",
    "- When training for a long time the chances of machine failure increases.\n",
    "- If there is a problem we would like to resume from an intermediate point\n",
    "\n",
    "### Solution\n",
    "\n",
    "- At the end of every epoch save the model state\n",
    "- If a machine failure occures we can resume from the saved state and restart\n",
    "- Make sure the full model state is saved not the just the model\n",
    "- Once training is complete and exported it is usually only the information required to make a prediciton is saved\n",
    "\n",
    "- Good to save information about the training loop as well e.g.\n",
    "    - Learning rate in a learning rate scheduler\n",
    "    - Batch number\n",
    "- Saving a full model state so that is can be resumed is called checkpointing\n",
    "- Model states changes with every batch but there is much overhead to save at every batch so do it at the end of each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example to save model state in pytorch\n",
    "\n",
    "```\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_stat_dict': model.state_dict(),\n",
    "    'optimiser_state_dict': optimiser.state_dict(),\n",
    "    'loss': loss,\n",
    "    ...\n",
    "}, PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why it Works\n",
    "\n",
    "- Most ML frameworks can result from a saved checkpoint\n",
    "- Checkpoints are designed for mainly for resilience, their availability however opens other use cases\n",
    "- Partially trained models are usually more generalisable that models created later iterations \n",
    "    - [See here](https://playground.tensorflow.org/)\n",
    "    \n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "- Saving checkpoints allows us to implement early stopping and fine-tuning capabilities\n",
    "\n",
    "#### Early Stopping\n",
    "\n",
    "- Typically, the longer you train the lower your training loss\n",
    "- At a certain point the loss on the validation set may stoping decreasing\n",
    "- If you being to overfit the error on the validation set may increase\n",
    "- Handy to look at the validation error at the end of every epoch and stop the training process when the validation error is more than that of the previous epoch\n",
    "\n",
    "#### Checkpoint Selection\n",
    "\n",
    "- It's not uncommon for the validation error to decrease, increase slightly then decrease again, therefore early stopping of the validation loss increases may not be optimum\n",
    "- This is because the training initally focuses on common cases, then begings to look at rarer cases [Paper](https://arxiv.org/abs/1912.02292)\n",
    "- Therefore training should continue for a while longer\n",
    "\n",
    "#### Regularisation\n",
    "\n",
    "- Instead of early stopping or checkpoint selection, it can be helpful to add L2 regularisation to your model so that the validation error does not increase.\n",
    "- Instead, both the training loss and the validation error plateau. We term such a training loop where both training and validation plateau a well-behaved training loop\n",
    "\n",
    "- Regularisation might be better than early stopping is that regularisation allows you to use the entire dataset to change the weights of the model\n",
    "- With early stopping you have to decide where to stop on the validation set so some data is wasted in this set\n",
    "\n",
    "#### Two-Splits\n",
    "\n",
    "- Recommended to split data into two parts: a training set and evaluation set\n",
    "- Evaluation set plays the part of the test dataset during training\n",
    "\n",
    "- A larger training dataset allows for a more complex model and the more accurate the model can get\n",
    "- Using regularisation rather than early stopping or checkpoint selection allows you to use a larget training dataset\n",
    "- During the experimentation phase e.g. hyperparm and model architecture exploring, early stopping should be turned off\n",
    "- This ensures the model has enough capacity to learn the predictive patterns.\n",
    "\n",
    "- When training a model for prod be prepped for continuous evaluation and model retraining\n",
    "\n",
    "#### Fine Tuning\n",
    "\n",
    "- A well-behaved training loop will get to the neighborhood of the optimal error quickly on the basis of the majority of your data, then slowly converge toward the lowest error by optimising for edge cases\n",
    "- When new data comes you want to emphasize the fresh data not the corner cases. In this cases we are better off resuming training where the model had generalised to the majority of the data and not when it optimised for edge cases. Essentially resume training **not** from the last checkpoint but from and earlier one.\n",
    "- It's not always necessary to start from an earlier checkpoint. In some cases the final checkpoint used to serve the model can be used as a warm start for another model training iteration. Still, starting from an earlier checkkpoint tends to provide better generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 13: Transfer Learning\n",
    "\n",
    "- Take a previously trained model, freeze the weights and incorporate the nontrainable layers into a new model to solve a similar problem\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Large training datasets not always available \n",
    "- Pre-trained models may not be possible to use out of the box for out particular use case\n",
    "\n",
    "### Solution\n",
    "\n",
    "- We take a model that has been trained on the same type of data (e.g. images) for a similar task and apply it to a specialised task using our own data\n",
    "- Use a pre-trained model that was performed to the same type of task e.g. use a pre-trained image classifier if you want to classify new images\n",
    "\n",
    "- Idea behind transfer learning is that you utilise the weights and layers from a model trained in the same domian as your prediction task\n",
    "- In mode deep learning models, the final layer contains the classification label or output specific to your prediction task\n",
    "- In transfer learning we remove the last layer, freeze the model weights and replace the final layer with the output for our specialised prediction task before continuing to train \n",
    "- Typically, the penultimate layer of the model (layer before the output) is chosen as the bottleneck layer\n",
    "\n",
    "#### Bottleneck Layer\n",
    "\n",
    "- This layer typically represents the input in the lowest dimensionality space\n",
    "    - This doesn't always have to be the last fully connected layer. It can also be the last max pooling layer before the fully connected layers\n",
    "- A general rule of thumb, the bottleneck layer is typically the last, lowest-dimensionality, flattened layer before the flattening operation\n",
    "- Because these layers represent features in reduced dimensionality, bottleneck layers are conceptually similar to embeddings\n",
    "- An embedding layer is essentially a lookup table of weights, mapping a particular feature to some dimension in vector space\n",
    "    - The main difference is that the weights in an embeding layer can be trained, whereas all the layers leading up to and including the bottleneck layer have their weights frozen\n",
    "    \n",
    "#### Implementing Transfer Learning\n",
    "\n",
    "- This can be done by loading a pre-trained model, removing the layers after the bottleneck and adding a new final layer with your own data labels\n",
    "- E.g. let's look at the final layers of the VGG 19 model\n",
    "\n",
    "```\n",
    "- block5_conv3 (Conv2D) (14, 14, 512)\n",
    "- block5_conv4 (Conv2D) (14, 14, 512)\n",
    "- block5_pool (MaxPooling2D) (7, 7, 512)\n",
    "- Flatten (Flatten) (25088)\n",
    "- fc1 (Dense) (4096)\n",
    "- fc2 (Dense) (4096)\n",
    "- predictions (Dense) (1000)\n",
    "```\n",
    "\n",
    "- The last layer has an output of 1000 one for each class it's trying to predict\n",
    "    - In this situation we can remove the layers from `block5_pool (MaxPooling2D) (7, 7, 512)`\n",
    "    - The last layer of the model will then be `block5_conv4 (Conv2D) (14, 14, 512)`\n",
    "    - Lets add a global average pooling 2D layer, followed by a dense layer with the output size equal to the number of classes we want to predict\n",
    "    - The new model will look like:\n",
    "\n",
    "```\n",
    "- block5_conv3 (Conv2D) (14, 14, 512)\n",
    "- block5_conv4 (Conv2D) (14, 14, 512)\n",
    "- GlobalMaxPooling2D (GlobalMaxPooling2D) (512) <-- Global max pooling is different to max pooling. Global compresses a 3D shape into a 1D shape\n",
    "- Prediction (Dense) (5)\n",
    "```\n",
    "\n",
    "- The new model now predicts 5 classes\n",
    "- The important piece here is that the only trainable parameters are the ones after our bottleneck layer.\n",
    "- We now train the model on our own data\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "- With CNN learning in heirarchical\n",
    "    - First layers learn edges and shapes present in images\n",
    "    - Next layers learn to understand groups of edges \n",
    "    - The final layers learn to piece together these groups of edges to develop an understanding of different features in the image\n",
    "    \n",
    "- We can visualise the [process](https://arxiv.org/pdf/1311.2901.pdf)\n",
    "\n",
    "### Trade-Offs and Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
