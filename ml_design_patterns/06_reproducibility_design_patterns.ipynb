{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Design Patterns\n",
    "\n",
    "- Unit testing produces a deterministic output\n",
    "- This reproducibility is difficult in ML\n",
    "- Many models start with random values which are adjusted during training\n",
    "- It is possible to set a `random_state` which returns the same results each time\n",
    "- Beyond random seed many other artifacts need to be fixed in order to ensure reproducibility during training\n",
    "- ML also has multiple stages e.g. training, deployment etc... need to ensure these are reproducible as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 21: Transform\n",
    "\n",
    "- This pattern makes moving ML inot production easier by keeping inputs, features and transforms carfully separate\n",
    "\n",
    "### Probelm\n",
    "\n",
    "- Inputs to ML models are the not the features that the ML model uses\n",
    "- e.g. words are not directly used in models and need to be converted into some numerical form\n",
    "- Need to keep track of transforms otherwise we cant reproduce in prod\n",
    "\n",
    "\n",
    "- Training serving skew is casued by differences between trainijg and prod.\n",
    "- e.g. if wednesday is encoded into a 3 during training in prod we need to know that this is the case. Some libraries may encoded tuesday into a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- Explcitily capture the transformations applied to convert the model inputs into features\n",
    "- In sklearn you would pickle the transformer\n",
    "- Load in the pickled transformer and use this to transform new data into the required model inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 22: Repeatable Splitting\n",
    "\n",
    "- Ensure that sampling is repeatable and reproducible\n",
    "- Needed for a determistic hash function to split data into train, test and validation\n",
    "\n",
    "### Problem \n",
    "\n",
    "- Not good to randomly split rows in data\n",
    "    - It's rare that rows in data are independent\n",
    "    - e.g. model to predict flight delays. A day with lots of delays, a lot of flights on that day will have delays. Having some of these flights in training and some in test is not right and leads to data leakage. The leakage is due to correlated rows and this is a common problem. We need to avoid this during ML training.\n",
    "- Randomly splitting rows is also bad as it can lead to a different dataset each time which is bad for reproducibility and comparing models.\n",
    "- Set a random seed on the splitting or store the data in advance to get around this\n",
    "\n",
    "- For ML we need repeatable splitting and ensure correlated rows fall into the same split.\n",
    "    - e.g. don't want flights on 28th June 2022 in both train and test set\n",
    "    \n",
    "### Solution\n",
    "\n",
    "- Find the column that caputres the correlation relationship between rows\n",
    "    - In our example of plane delays it would be the `date` column\n",
    "- Hash the values in the column and apply a modulo to split the data into train, validation and test\n",
    "    - All same flights for a given day will have the same hash value because they will occur on the same `date`\n",
    "    - This make it repeatable\n",
    "\n",
    "- Take the modulo of the has to ten e.g. `% 10`\n",
    "    - If the value is < 8 it goes into training\n",
    "    - If the value is =8 it goes into validation\n",
    "    - If the value is =9 it goes into test\n",
    "    - This is how we get the 80%, 10% and 10% split for training, validation and test sets\n",
    "    \n",
    "- The `date` column here make sense to split on because:\n",
    "    - Rows at the same date tend to be correlated\n",
    "    - `date` is not an input into the model. We can extract other features instead such as day of week.\n",
    "    - We must have enough `date` values. We are computing the hash and finding the modulo with respect to 10, we need at least 10 unique hash values. The more unique values we have the better. A rule of thumb: number of unique values should be 3 - 5x the denominator for the modulo. In our case we want 40 or so unique dates\n",
    "    - Labels have to well distributed among dates. If all delays happen on July 1st and no delays on other days of the year, this wouldn't work since the split dataset will be skewed\n",
    "    \n",
    "_We can automate checking whether the label distributions are similar across the three datasets by using the Kolomogorov-Smirnov test. Just plot the cumulative distributions functions of the label in the three datasets and find the maximum distance between each pair. The smaller the maximum distance, the better the split_\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Random Split\n",
    "\n",
    "- If the rows are not correlated we can do a random split which is repeatable\n",
    "- If there is no natural column to split by, hash the entire row of data by converting it to a string and take the modulo like above\n",
    "- If there are duplicate rows they will have the exact same hash and end up in the same split. If this is not what you desire add a unique ID column\n",
    "\n",
    "#### Split on Multiple Columns\n",
    "\n",
    "- If a combination of rows captures when two rows are correlated, simply concatenate the fields before computing the hash\n",
    "- This would help in the airport scenario above where we would concatenate the airport name and date, hash and then the modulo.\n",
    "\n",
    "#### Sequential Split\n",
    "\n",
    "- Time series will need sequential splits\n",
    "    - e.g. train on past 45 days to predict the next 14 days\n",
    "- Such splits also useful of fast changing environments\n",
    "    - e.g. bad actors in fraud quickly adapt to the fraud algorithm. As a result the model needs to be repeatably re-trained on  the lastest data.\n",
    "        - Not sufficient to generate random split from historical data because the goal is to predict behaviour that the bad actors will exhibit in the future\n",
    "        - The indirect goal is the same as that of a time-series model in that a good model will be able to train on historical data and predicit future fraud. The data has to be split sequentially in terms of time to corrrectly evaluate this.\n",
    "- Another place where sequential splits make sense is high correlations between successive times.\n",
    "    - e.g. weather forecasting.\n",
    "    - The weather on consecutive days is highly correlated\n",
    "    - Not reasonable to put Jan 14th in training set and Jan 15th in test set because there will be leakage\n",
    "\n",
    "#### Stratified Splits\n",
    "\n",
    "- Simple enough to google\n",
    "\n",
    "#### Unstructured Data\n",
    "\n",
    "- Photos, videos, text\n",
    "- Use meta data to split the samples\n",
    "    - Carefull of leakage e.g. videos shot on the same day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 23: Birdged Schema\n",
    "\n",
    "- Provides a way to adapt the data used to train a model from its older, original data schema to a newer, better data\n",
    "- Useful when input provider makes improvements to their feed it can take time to update the schema to the new data\n",
    "- This patterns allows us to use as much of the new data as is available, but augment it with some of the older data to improve accuracy\n",
    "\n",
    "### Probelm\n",
    "\n",
    "- We have a point of sale application that determines how much to tip the delivery driver\n",
    "- One of the inputs is \"cash\" or \"card\"\n",
    "- Card is then updated to now be \"gift card\", \"debit card\", \"credit card\"\n",
    "- This is valuable because the tipping behaviour varies between the cards\n",
    "- At prediction time this information is already available and we would like to use it as soon as possible\n",
    "- Cannot train a model on exclusively new data because the quantity of new data is rather small\n",
    "\n",
    "### Solution\n",
    "\n",
    "- The solution is to bridge the schema of the old data to match the new data\n",
    "- Then we train an ML model. using as much of the new data as is available and augment it with older data\n",
    "\n",
    "#### Bridged Schema\n",
    "\n",
    "- In the new schema the card category is much more granual (\"gift card\", \"debit card\", \"credit card\")\n",
    "- We know that as transaction coded as card will in the old data would have been on of these types but the actual type was not recorded\n",
    "- It is possible to bridge the schema probabilistically or statically\n",
    "- Static is recommended \n",
    "\n",
    "#### Probabilistic Method\n",
    "\n",
    "- Estimated from newer data that 10% are gift cards, 30% are debit cards and 60% are credit cards\n",
    "- Each time an older training example is loaded we generate a random number between \\[0, 100\\)\n",
    "    - < 10 = gift card\n",
    "    - \\[10, 40\\) = debit card\n",
    "    - >= 40 = credit card\n",
    "- Provided we train for enough epochs, any training example would be presented as all three categories, but proportional to the acutal frequency of occurrence\n",
    "- New training examples will use the actual recorded value\n",
    "\n",
    "\n",
    "- Justification is we treat each older example as having happened hundreds of times \n",
    "- As the trainer goes through the data, in each epoch, we simulate one of those instances\n",
    "- In the simulation, we expect that 10\\% of the time that a card was used the transaction would have occurred with a gift card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Method\n",
    "\n",
    "- Categorical variables usually one-hot-encoded\n",
    "- If we train for long enough the average one-hot encoded value presented would be `[0, 0.1, 0.3, 0.6]` where the first value is the cash category\n",
    "\n",
    "\n",
    "- To bridge the older data to the newer schema we can transform the older categorial data into this representation where we insert the a priori probaiblity of the new classes as estimated from the training data.\n",
    "- Newer data on the other hand will have `[0, 0, 1, 0]` for a transaction that is know to have been paid by a debit card\n",
    "- Static method is preffered because it is effectively what happens if the proabilistic method runs for a long enough.\n",
    "- It is also simpler to implement since every card payment from the old data will ahve the exact same value (`[0, 0.1, 0.3, 0.6]`)\n",
    "- We can update the older data in one line of code\n",
    "- This is also compuationally less expensive \n",
    "\n",
    "#### Augmented Data\n",
    "\n",
    "- If 95% of old data is in the old schema and 5% in the new what should be the data split?\n",
    "- Models need to make predictions on new unseen data. The unseen data in this case will exclusively be in the new schema.\n",
    "- Could set aside 2,000 examples in the new schema and add this to your evaluation set along with some from the bridged schema.\n",
    "\n",
    "- How do we know 2,000 examples is enough? We can test this by evaluating the metric of the current production model (trained on old schema) on subsets of its evaluation datasest and determine how large the subset has to be before the evaluation metric is consistent.\n",
    "- Start with a randomly selected sample size of 100 and increase in steps of 100 to 5000. This is only for the new datapoints\n",
    "    - At each step take a random sample and calculate the evaluation metric\n",
    "    - Re-run each step 25 times to calculate the standard deviation of the metric\n",
    "    - Plot a graph where y-axis is evluation metric, x-axis is the evaluation size and the line drawn is the standard deviation of the evaluation metric at each step size\n",
    "    - Where line plateaus is where the ideal number of data points in evaluation size\n",
    "- Down side is we don't know how many older examples we need\n",
    "    - This is a hyperparameter we will need to tune\n",
    "- For best results, use the smalled number of older examples that we can get away with\n",
    "- Over time as the number of new examples grows, we'll rely less and less on bridges examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-Offs and Alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
