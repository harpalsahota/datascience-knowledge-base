{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Design Patterns\n",
    "\n",
    "- Unit testing produces a deterministic output\n",
    "- This reproducibility is difficult in ML\n",
    "- Many models start with random values which are adjusted during training\n",
    "- It is possible to set a `random_state` which returns the same results each time\n",
    "- Beyond random seed many other artifacts need to be fixed in order to ensure reproducibility during training\n",
    "- ML also has multiple stages e.g. training, deployment etc... need to ensure these are reproducible as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 21: Transform\n",
    "\n",
    "- This pattern makes moving ML inot production easier by keeping inputs, features and transforms carfully separate\n",
    "\n",
    "### Probelm\n",
    "\n",
    "- Inputs to ML models are the not the features that the ML model uses\n",
    "- e.g. words are not directly used in models and need to be converted into some numerical form\n",
    "- Need to keep track of transforms otherwise we cant reproduce in prod\n",
    "\n",
    "\n",
    "- Training serving skew is casued by differences between trainijg and prod.\n",
    "- e.g. if wednesday is encoded into a 3 during training in prod we need to know that this is the case. Some libraries may encoded tuesday into a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- Explcitily capture the transformations applied to convert the model inputs into features\n",
    "- In sklearn you would pickle the transformer\n",
    "- Load in the pickled transformer and use this to transform new data into the required model inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 22: Repeatable Splitting\n",
    "\n",
    "- Ensure that sampling is repeatable and reproducible\n",
    "- Needed for a determistic hash function to split data into train, test and validation\n",
    "\n",
    "### Problem \n",
    "\n",
    "- Not good to randomly split rows in data\n",
    "    - It's rare that rows in data are independent\n",
    "    - e.g. model to predict flight delays. A day with lots of delays, a lot of flights on that day will have delays. Having some of these flights in training and some in test is not right and leads to data leakage. The leakage is due to correlated rows and this is a common problem. We need to avoid this during ML training.\n",
    "- Randomly splitting rows is also bad as it can lead to a different dataset each time which is bad for reproducibility and comparing models.\n",
    "- Set a random seed on the splitting or store the data in advance to get around this\n",
    "\n",
    "- For ML we need repeatable splitting and ensure correlated rows fall into the same split.\n",
    "    - e.g. don't want flights on 28th June 2022 in both train and test set\n",
    "    \n",
    "### Solution\n",
    "\n",
    "- Find the column that caputres the correlation relationship between rows\n",
    "    - In our example of plane delays it would be the `date` column\n",
    "- Hash the values in the column and apply a modulo to split the data into train, validation and test\n",
    "    - All same flights for a given day will have the same hash value because they will occur on the same `date`\n",
    "    - This make it repeatable\n",
    "\n",
    "- Take the modulo of the has to ten e.g. `% 10`\n",
    "    - If the value is < 8 it goes into training\n",
    "    - If the value is =8 it goes into validation\n",
    "    - If the value is =9 it goes into test\n",
    "    - This is how we get the 80%, 10% and 10% split for training, validation and test sets\n",
    "    \n",
    "- The `date` column here make sense to split on because:\n",
    "    - Rows at the same date tend to be correlated\n",
    "    - `date` is not an input into the model. We can extract other features instead such as day of week.\n",
    "    - We must have enough `date` values. We are computing the hash and finding the modulo with respect to 10, we need at least 10 unique hash values. The more unique values we have the better. A rule of thumb: number of unique values should be 3 - 5x the denominator for the modulo. In our case we want 40 or so unique dates\n",
    "    - Labels have to well distributed among dates. If all delays happen on July 1st and no delays on other days of the year, this wouldn't work since the split dataset will be skewed\n",
    "    \n",
    "_We can automate checking whether the label distributions are similar across the three datasets by using the Kolomogorov-Smirnov test. Just plot the cumulative distributions functions of the label in the three datasets and find the maximum distance between each pair. The smaller the maximum distance, the better the split_\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Random Split\n",
    "\n",
    "- If the rows are not correlated we can do a random split which is repeatable\n",
    "- If there is no natural column to split by, hash the entire row of data by converting it to a string and take the modulo like above\n",
    "- If there are duplicate rows they will have the exact same hash and end up in the same split. If this is not what you desire add a unique ID column\n",
    "\n",
    "#### Split on Multiple Columns\n",
    "\n",
    "- If a combination of rows captures when two rows are correlated, simply concatenate the fields before computing the hash\n",
    "- This would help in the airport scenario above where we would concatenate the airport name and date, hash and then the modulo.\n",
    "\n",
    "#### Sequential Split\n",
    "\n",
    "- Time series will need sequential splits\n",
    "    - e.g. train on past 45 days to predict the next 14 days\n",
    "- Such splits also useful of fast changing environments\n",
    "    - e.g. bad actors in fraud quickly adapt to the fraud algorithm. As a result the model needs to be repeatably re-trained on  the lastest data.\n",
    "        - Not sufficient to generate random split from historical data because the goal is to predict behaviour that the bad actors will exhibit in the future\n",
    "        - The indirect goal is the same as that of a time-series model in that a good model will be able to train on historical data and predicit future fraud. The data has to be split sequentially in terms of time to corrrectly evaluate this.\n",
    "- Another place where sequential splits make sense is high correlations between successive times.\n",
    "    - e.g. weather forecasting.\n",
    "    - The weather on consecutive days is highly correlated\n",
    "    - Not reasonable to put Jan 14th in training set and Jan 15th in test set because there will be leakage\n",
    "\n",
    "#### Stratified Splits\n",
    "\n",
    "- Simple enough to google\n",
    "\n",
    "#### Unstructured Data\n",
    "\n",
    "- Photos, videos, text\n",
    "- Use meta data to split the samples\n",
    "    - Carefull of leakage e.g. videos shot on the same day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 23: Birdged Schema\n",
    "\n",
    "- Provides a way to adapt the data used to train a model from its older, original data schema to a newer, better data\n",
    "- Useful when input provider makes improvements to their feed it can take time to update the schema to the new data\n",
    "- This patterns allows us to use as much of the new data as is available, but augment it with some of the older data to improve accuracy\n",
    "\n",
    "### Probelm\n",
    "\n",
    "- We have a point of sale application that determines how much to tip the delivery driver\n",
    "- One of the inputs is \"cash\" or \"card\"\n",
    "- Card is then updated to now be \"gift card\", \"debit card\", \"credit card\"\n",
    "- This is valuable because the tipping behaviour varies between the cards\n",
    "- At prediction time this information is already available and we would like to use it as soon as possible\n",
    "- Cannot train a model on exclusively new data because the quantity of new data is rather small\n",
    "\n",
    "### Solution\n",
    "\n",
    "- The solution is to bridge the schema of the old data to match the new data\n",
    "- Then we train an ML model. using as much of the new data as is available and augment it with older data\n",
    "\n",
    "#### Bridged Schema\n",
    "\n",
    "- In the new schema the card category is much more granual (\"gift card\", \"debit card\", \"credit card\")\n",
    "- We know that as transaction coded as card will in the old data would have been on of these types but the actual type was not recorded\n",
    "- It is possible to bridge the schema probabilistically or statically\n",
    "- Static is recommended \n",
    "\n",
    "#### Probabilistic Method\n",
    "\n",
    "- Estimated from newer data that 10% are gift cards, 30% are debit cards and 60% are credit cards\n",
    "- Each time an older training example is loaded we generate a random number between \\[0, 100\\)\n",
    "    - < 10 = gift card\n",
    "    - \\[10, 40\\) = debit card\n",
    "    - >= 40 = credit card\n",
    "- Provided we train for enough epochs, any training example would be presented as all three categories, but proportional to the acutal frequency of occurrence\n",
    "- New training examples will use the actual recorded value\n",
    "\n",
    "\n",
    "- Justification is we treat each older example as having happened hundreds of times \n",
    "- As the trainer goes through the data, in each epoch, we simulate one of those instances\n",
    "- In the simulation, we expect that 10\\% of the time that a card was used the transaction would have occurred with a gift card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static Method\n",
    "\n",
    "- Categorical variables usually one-hot-encoded\n",
    "- If we train for long enough the average one-hot encoded value presented would be `[0, 0.1, 0.3, 0.6]` where the first value is the cash category\n",
    "\n",
    "\n",
    "- To bridge the older data to the newer schema we can transform the older categorial data into this representation where we insert the a priori probaiblity of the new classes as estimated from the training data.\n",
    "- Newer data on the other hand will have `[0, 0, 1, 0]` for a transaction that is know to have been paid by a debit card\n",
    "- Static method is preffered because it is effectively what happens if the proabilistic method runs for a long enough.\n",
    "- It is also simpler to implement since every card payment from the old data will ahve the exact same value (`[0, 0.1, 0.3, 0.6]`)\n",
    "- We can update the older data in one line of code\n",
    "- This is also compuationally less expensive \n",
    "\n",
    "#### Augmented Data\n",
    "\n",
    "- If 95% of old data is in the old schema and 5% in the new what should be the data split?\n",
    "- Models need to make predictions on new unseen data. The unseen data in this case will exclusively be in the new schema.\n",
    "- Could set aside 2,000 examples in the new schema and add this to your evaluation set along with some from the bridged schema.\n",
    "\n",
    "- How do we know 2,000 examples is enough? We can test this by evaluating the metric of the current production model (trained on old schema) on subsets of its evaluation datasest and determine how large the subset has to be before the evaluation metric is consistent.\n",
    "- Start with a randomly selected sample size of 100 and increase in steps of 100 to 5000. This is only for the new datapoints\n",
    "    - At each step take a random sample and calculate the evaluation metric\n",
    "    - Re-run each step 25 times to calculate the standard deviation of the metric\n",
    "    - Plot a graph where y-axis is evluation metric, x-axis is the evaluation size and the line drawn is the standard deviation of the evaluation metric at each step size\n",
    "    - Where line plateaus is where the ideal number of data points in evaluation size\n",
    "- Down side is we don't know how many older examples we need\n",
    "    - This is a hyperparameter we will need to tune\n",
    "- For best results, use the smalled number of older examples that we can get away with\n",
    "- Over time as the number of new examples grows, we'll rely less and less on bridges examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Union Schema\n",
    "\n",
    "- Could just take a union of both data sets, this would make the new and old datasets compatible so the possible values would be `[cash, card, debit card, credit card, gift card]`\n",
    "- But the new predictions won't have `card` as a value and thus breaks the schema!\n",
    "\n",
    "#### Cascade Method\n",
    "\n",
    "- Impute the value, could take mean.\n",
    "- Take priori frequencies as shown above\n",
    "\n",
    "\n",
    "- Can add a cascade model. See Design pattern 8.\n",
    "    - Train a model on the new data with card types\n",
    "    - Output of this model is used to train the second model\n",
    "    \n",
    "#### Handling New Features\n",
    "\n",
    "- Bridging might be needed when the input provider adds extra information\n",
    "- If we have new features that we want to use immediately, we should bridge the older data (where this new feature will be missing) by imputing a value for this feature\n",
    "- To imput we should try to use:\n",
    "    - The mean value of the feature if the feature is numeric and normally distributed\n",
    "    - The median if the feature is numeric and skewed or has lots of outliers\n",
    "    - Median value of the feature if the feature is categorical and sortable\n",
    "    - Mode of the feature if the featue is categorical and not sortable \n",
    "    - Frequency of the feature being true if it's boolean\n",
    "    \n",
    "- Taking the example of taxi journeys if the new feature is number of idle minutes we could use the median value \n",
    "- If the feature is a boolean for raining or not raining it's imputed value can be something line 0.02% if it rains 2% of the time in the new data\n",
    "\n",
    "- The cascade pattern approach remains viable for all these cases, but a static imputation is often simplier and often sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 24: Windowed Inference\n",
    "\n",
    "- Handles instances where the model requires an ongoing sequence of instances in order to run inference\n",
    "- This pattern is useful when a ML model requires features that need to computed from aggregates over time windows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "- Imagine we have flight data looking at arrival delays for flights\n",
    "- The arrival delays will naturally exhibit variability\n",
    "- But it still should be possible to note unusually large arrival delays\n",
    "- The definition of \"unusual\" will vary by context\n",
    "    - Early in the day flights are delayed less\n",
    "    - In the afternoon flights are delayed more\n",
    "    - The context here is \"time of day\"\n",
    "\n",
    "\n",
    "- Determining a specific delay in anomalous depends on a time context.\n",
    "    - e.g. arrival delays observed over the past two hours\n",
    "- To determine that a delay is anomalous requires that we first sort the dataframe based on time\n",
    "- We then apply an anomaly detection function to sliding windows of two hourse\n",
    "- The function to detect the anomaly can be complex but a simple thing to do is highlight values 4 standard deviations from the mean in the two hour window\n",
    "\n",
    "\n",
    "- This works on training data because the entire dataframe is at hand\n",
    "- When running inference we will not have the entire dataframe available\n",
    "- In prod, we will be reciving flight information one by one as each flight arrives\n",
    "- All that we will have is a single delay value at a timestamp \n",
    "    - `2022-01-24 07:30:00, 43.0`\n",
    "- Given that the flight above is 43 minutes delayed is that unusual or not?\n",
    "- To carry out inference on a flight we only need the features of that flight\n",
    "- In this case, the model requires information about all flights to the airport between `05:30` and `07:30`\n",
    "- Is it not possible to carry out inference one flight at a time. We need to somehow provide the model information about all the previous flights\n",
    "\n",
    "\n",
    "- How do we carry out inference when the model requires not just one instance, but a sequence of instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "- Carry out stateful stream processng, that is processing that keeps track of the model state through time\n",
    "\n",
    "\n",
    "- Sliding window applied to flight arrival data.\n",
    "    - The sliding window will be over 2 hours but the window can be closed more often such as every 10 mins\n",
    "    - In such a case, aggregate values will be calculated every 10 mins over the previous 2 hours\n",
    "- Internal model state (this could be a list of flights) is updated with flight information every time a new flight arrives, thus building a 2-hour historical record of flight data\n",
    "- Every time a window is closed (e.g. every 10 mins), a time series ML model is trained on the 2-hour list of flights. This model is then used to predict future flight delays and the confidence bounds of such predictions\n",
    "- Time series model parameters are externalised into a state variable. Could use ARIMA of LSTM and in such cases, the model params would be ARIMA coefficients or the LSTM weights.\n",
    "    - To keep code understanable, we will use a zero-order regression model, and so our model. parameters will be the average flight delay and the variance of the flight delays over the two-hour window\n",
    "- When a flight arrives, its arrival delay can be classified as anomalous or not using the externalised model state\n",
    "- Every time the window is closed, the out put is extacted. The output here is the externalised model state and consists of the model parameters.\n",
    "    - The params here are the mean delay time and acceptable deviations which is 4 * the standard deviation\n",
    "    - Any new flight coming in compared against the model state and if outside the acceptable deviation is considered anomalous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-Offs and Alternatives\n",
    "\n",
    "### High-throughput data stream\n",
    "- If we revieve 5000 items a second then the in-memory dataframe over 10 mins will contain 3 million rows \n",
    "- The memory requirments can become considerable\n",
    "\n",
    "\n",
    "- Storing all the records in order to compute the model parameters at the end of the window can become problematic\n",
    "- When the data stream is high throughput, it becomes important to be able to update the model parameters with each element\n",
    "- An online learning model maybe best here (see page 279 in book)\n",
    "\n",
    "### Batching Requests\n",
    "- If model deployed in cloud but the client is embedded in the device \n",
    "- Sending inference requests one by one to a cloud might be overwhelming \n",
    "- Collect a number of requests\n",
    "- Then service those requests as a batch \n",
    "- See Design Pattern 19: Two-Phase Prediction\n",
    "- This is suitable for latency tolerant use cases\n",
    "- If we collect inputs for 5 minutes then the client will have to tolerant for up to 5 minutes delays to get back predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 25: Workflow Pipeline\n",
    "\n",
    "- The workflow pipeline addresses the problem of creating an end-to-end reproducible pipeline by containerising and orchestrating the steps in our machine learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "- A data scientist is able to run, data processing, training and model deployment steps in an end-to-end fashion with a single notebook\n",
    "- However, as each step in the ML pipeline becomes more complex and more people within the organisation want to contribute to this code base, running these steps from a single notebook does not scale \n",
    "\n",
    "\n",
    "- In monolithic aapplications the applications logic is handled by a single program.\n",
    "- To test a small feature in a monolithic app we mus run the entire program. Same goes deploying or debugging \n",
    "- Deploying a small bug fix means requires deploying the entire application \n",
    "- When the codebase is inextricably linked, it becomes difficult for individual developers to debug errors and word independently on different parts of the application\n",
    "- Recently monolithic apps have been replaced in favor of a microservices architecture where individual pieces of business logic are built and deployed as isolated packages of code.\n",
    "- With microservices, a large applications is split into smaller, more manageable parts so that developers can build, debug and deploy pieces of an applications independelty\n",
    "\n",
    "\n",
    "- When someone is building an ML model on their own, a \"monolithic\" approach may be faster to iterate on.\n",
    "- It also works because one person is actively involved in devloping and maintaining each piece of the pipeline e.g.\n",
    "    - Data collection\n",
    "    - Data validation\n",
    "    - Data preprocessing\n",
    "    - Model building\n",
    "    - Training and validation\n",
    "    - Model deployment\n",
    "- When scaling this workflow different parts of the organisation may be responsible for different steps\n",
    "- To scale the ML workflow, we need a way for the team building out the model to run trials independently of the data processing step\n",
    "- We also need to keep track of performance for each step of the pipeline and manage the output files generated by each part of the process\n",
    "\n",
    "\n",
    "- When the development of each step is complete, we'll want to schedule operations like retraining, or create event-triggered pipeline runs that are invoked in repsone to changed in your environment\n",
    "    - e.g. new training data added to the bucket\n",
    "- In such cases it'll be necessary for the solution to allow us to run the entire workflow from end-to-end in on call while still being able to track output and trace errors from individual steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
