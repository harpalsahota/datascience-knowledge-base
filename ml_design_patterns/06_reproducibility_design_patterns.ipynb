{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Design Patterns\n",
    "\n",
    "- Unit testing produces a deterministic output\n",
    "- This reproducibility is difficult in ML\n",
    "- Many models start with random values which are adjusted during training\n",
    "- It is possible to set a `random_state` which returns the same results each time\n",
    "- Beyond random seed many other artifacts need to be fixed in order to ensure reproducibility during training\n",
    "- ML also has multiple stages e.g. training, deployment etc... need to ensure these are reproducible as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 21: Transform\n",
    "\n",
    "- This pattern makes moving ML inot production easier by keeping inputs, features and transforms carfully separate\n",
    "\n",
    "### Probelm\n",
    "\n",
    "- Inputs to ML models are the not the features that the ML model uses\n",
    "- e.g. words are not directly used in models and need to be converted into some numerical form\n",
    "- Need to keep track of transforms otherwise we cant reproduce in prod\n",
    "\n",
    "\n",
    "- Training serving skew is casued by differences between trainijg and prod.\n",
    "- e.g. if wednesday is encoded into a 3 during training in prod we need to know that this is the case. Some libraries may encoded tuesday into a 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "- Explcitily capture the transformations applied to convert the model inputs into features\n",
    "- In sklearn you would pickle the transformer\n",
    "- Load in the pickled transformer and use this to transform new data into the required model inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 22: Repeatable Splitting\n",
    "\n",
    "- Ensure that sampling is repeatable and reproducible\n",
    "- Needed for a determistic hash function to split data into train, test and validation\n",
    "\n",
    "### Problem \n",
    "\n",
    "- Not good to randomly split rows in data\n",
    "    - It's rare that rows in data are independent\n",
    "    - e.g. model to predict flight delays. A day with lots of delays, a lot of flights on that day will have delays. Having some of these flights in training and some in test is not right and leads to data leakage. The leakage is due to correlated rows and this is a common problem. We need to avoid this during ML training.\n",
    "- Randomly splitting rows is also bad as it can lead to a different dataset each time which is bad for reproducibility and comparing models.\n",
    "- Set a random seed on the splitting or store the data in advance to get around this\n",
    "\n",
    "- For ML we need repeatable splitting and ensure correlated rows fall into the same split.\n",
    "    - e.g. don't want flights on 28th June 2022 in both train and test set\n",
    "    \n",
    "### Solution\n",
    "\n",
    "- Find the column that caputres the correlation relationship between rows\n",
    "    - In our example of plane delays it would be the `date` column\n",
    "- Hash the values in the column and apply a modulo to split the data into train, validation and test\n",
    "    - All same flights for a given day will have the same hash value because they will occur on the same `date`\n",
    "    - This make it repeatable\n",
    "\n",
    "- Take the modulo of the has to ten e.g. `% 10`\n",
    "    - If the value is < 8 it goes into training\n",
    "    - If the value is =8 it goes into validation\n",
    "    - If the value is =9 it goes into test\n",
    "    - This is how we get the 80%, 10% and 10% split for training, validation and test sets\n",
    "    \n",
    "- The `date` column here make sense to split on because:\n",
    "    - Rows at the same date tend to be correlated\n",
    "    - `date` is not an input into the model. We can extract other features instead such as day of week.\n",
    "    - We must have enough `date` values. We are computing the hash and finding the modulo with respect to 10, we need at least 10 unique hash values. The more unique values we have the better. A rule of thumb: number of unique values should be 3 - 5x the denominator for the modulo. In our case we want 40 or so unique dates\n",
    "    - Labels have to well distributed among dates. If all delays happen on July 1st and no delays on other days of the year, this wouldn't work since the split dataset will be skewed\n",
    "    \n",
    "_We can automate checking whether the label distributions are similar across the three datasets by using the Kolomogorov-Smirnov test. Just plot the cumulative distributions functions of the label in the three datasets and find the maximum distance between each pair. The smaller the maximum distance, the better the split_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
