{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing Patterns for Resilient Serving\n",
    "\n",
    "- Software deployed into production environments is expected to be resilient and require little in the way of human intervention to keep it running\n",
    "- Stateless serving\n",
    "    - This patter allows to scale and handle thousands or even millions of predictions requests per second\n",
    "- Batch Serving\n",
    "    - Asynchronously handle occasional or periodic requests for millions to billions of predictions\n",
    "    \n",
    "- Continued model evaluation\n",
    "    - Patterns that handle common problem of detecting when a deployed model is no longer fit-for-purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 16: Stateless Serving Function\n",
    "\n",
    "- A statless function is a function whose outputs are determined purely by its inputs\n",
    "- A function that maintains a counter of the number of times it's been called is stateful\n",
    "    - Can be done via class variables\n",
    "    \n",
    "- Because stateless components don't have any state, they can be shared by multiple clients\n",
    "- Servers typically create a pool of statelss components and use them to service client requests as they come in\n",
    "- On the other hand, stateful components will need to represent each clients conversational state\n",
    "- Lifecycle of stateless components needs to be mandages by the serve\n",
    "    - e.g. initialised on clients first request and then destroyed when the client terminates or times out\n",
    "    - Becuase of these factors stateless components are highly scalable\n",
    "    - Where as stateful components are expensive and difficult to manage\n",
    "- Web applications transfer state from the clients to the server with each call\n",
    "\n",
    "- In ML a lot of state is captured during training\n",
    "- When the model is exported / saved we reqiuire the model framework to keep track of these stateful variables\n",
    "\n",
    "- When stateless functions are used, it simplifies the server code and makes it more scalable but can make client code more complicated\n",
    "    - e.g. A spelling correction model has to know the previous few words in order to correct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "- Image we have a deep learning model trained (kera flavour)\n",
    "- Model makes predictions on how positive the reviews are for a film, therefore has a embedding layer\n",
    "- We use this model to make predictions via the `predict` call\n",
    "\n",
    "- There are several problems carrying out inference by calling `model.predict()` on an in memory object \n",
    "    - Loaded enitre keras model into memory, with all embeddings and layers this can be very large for deep learning models\n",
    "    - The preceding architechture imposes limits on the latency that can be achieved because calls to the `predict()` method have to be sent one by one\n",
    "    - Model input and output is most effective for training and may not be user friendly. The model may output [logits](https://en.wikipedia.org/wiki/Logit) but clients may want the sigmoid of that so that the output range is between 0 and 1. This can then me interpreted more as a probability. Model may have also been trained on compressed binary records where as the input format from the client may be a JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Export the model into a format that captures the mathmatical core of the model and is programming language agnostic\n",
    "2. In the production system, the formula consisting of the \"forward\" calculations of the model is resorted as a stateless function\n",
    "3. The stateless function is deployed into a framework that provided a REST endpoint\n",
    "\n",
    "#### Model Export\n",
    "\n",
    "- Can use [ONNX](https://onnx.ai)\n",
    "- Learning rate, dropout etc... don't need to be saved. This is what ONNX does.\n",
    "\n",
    "#### Inference in Python\n",
    "\n",
    "- In production the model's formula is restored along with any other files as a stateless function\n",
    "- The function must conform to a specidic model signature with the input and output variable names and input types\n",
    "- The signature specifies what the input and output of the model is \n",
    "\n",
    "#### Create Webendpoint\n",
    "\n",
    "- Should define the serving function as a global variable or a singleton class so that it isn't reloaded in response to every request.\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "#### AutoScaling\n",
    "\n",
    "- Scaling to milltions of requests per second is a well understood engineering problem\n",
    "- Rather than building services unique to ML we can rely on decades of engineering work that has gone into building resilient web applications\n",
    "- Modern cloud services offer auto-scaling services\n",
    "- Some ML framworks have their own serving subsystem:\n",
    "    - PyTorch: TorchServe\n",
    "    - TensorFLow: TensorFlow Serving\n",
    "    \n",
    "#### Fully Managed\n",
    "\n",
    "- Some cloud providers also have the capability of serving models e.g. sagemaker\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Customer Serving Function\n",
    "\n",
    "- Use a custom function to return the desired result to the client\n",
    "\n",
    "#### Multiple Signatures\n",
    "\n",
    "- There can be inexpensive (e.g. sigmoid) and expensive functions to run at inference time\n",
    "- It would not be good to return both to the client with each request\n",
    "- The client must explicitily request the inference from the expensive function\n",
    "\n",
    "#### Prediction Library\n",
    "\n",
    "- Instead of deploying the serving function as a microservice that can be called via a REST API, implement the prediction code as a library function\n",
    "- The library would load the model the first time it is called\n",
    "- Developers who need to predict with the libarary can then include the library with their application\n",
    "\n",
    "- A library function is better alternative than a microservice if the model cannot be called over a netowrk for physical or performance reasons\n",
    "- The libary function also places the computational burden on the clience and this might be preferable from a budgetary standpoint\n",
    "\n",
    "- Draw back is that maintenance and updates of the model are difficult\n",
    "- All client code that uses the model will have to be updated to use the new version of the library\n",
    "- The more the model is updated the more attractive the microservice approach becomes\n",
    "- The libaray approach is also restircted to programming languages for which the libraries are written\n",
    "- Where as REST API opens up the model to applications written in any modern language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 17: Batch Serving\n",
    "\n",
    "- Carries out inference on a larger number of instances all at once\n",
    "\n",
    "### Problem\n",
    "\n",
    "- Predictions are carried out one at a time and on demand\n",
    "    - e.g. working out if a credit card transaction is fraudulent or not\n",
    "- When a model is typically deployed it is setup to process one instance\n",
    "- The serving framework is archictected to process an individual request synchronously and as quickly as possible\n",
    "- This is usually a microservice\n",
    "\n",
    "- There are circumstances where predictions need to carried out asynchronously over large volumes of data\n",
    "    - e.g. do we need to buy more stock? This can happen hourly or daily not when every time an item is sold\n",
    "- Attempting to take an endpoint capable only only handling a single request at a time and sending it millions of requests may overwhelm the model (DDOS)\n",
    "\n",
    "### Solution\n",
    "\n",
    "- Batch serving pattern used a distributed data processing infrastructure (MapReduce, Spark, BigQuery, Apache Beam) to carry out ML inference on a large number of instances asynchronously\n",
    "- Using distributed systems to carry out one-off predictions is not very efficient need to pass large volume of data to performe inferecne to make it worth while\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "- Stateless serving function is setup for low latency serving to support thousands of simultaneous queries\n",
    "- Using this for periodic process and be time consuming and expensive\n",
    "- If requests are not latency sensitive it is more cost-effective to use a distributed data processing architecture\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "- Batch serving pattern depends on the ability to split a task across multiple workers\n",
    "- Even though batch serving is used when latency is not a concern, it is possible to incorporate precomputed results and periodic refreshing to use this in scenarios where the space of possible prediction inputs is limited\n",
    "\n",
    "#### Batch and Stream Pipelines\n",
    "\n",
    "- Frameworks like Spark or Apache Beam are useful when the input needs precprocessing before it can be supplied to the model, outputs require postproceccing or if either are hard to express in SQL\n",
    "\n",
    "- Apache Beam is useful if the client code needs to maintain state \n",
    "    - e.g. time-windowed average\n",
    "    - Stop users making multiple comments on a post\n",
    "        - State needed to keep count of how many times a user commented on a post\n",
    "- We can do the distributed processing and maintain state with with Apache Beam\n",
    "\n",
    "#### Cached Results of Batch Serving\n",
    "\n",
    "- Compute and cache work ahead of time\n",
    "    - e.g. if we have 10 millions users and 10,000 items and we want the top 5 items per customer ranks, this would be not be feasible to do in near real time\n",
    "\n",
    "#### Lambda Architecture\n",
    "- A production ML system that supports both online serving and batch serving is called a [Lambda Architechture](https://oreil.ly/jLZ46)\n",
    "- Such a ML system allows pracitioners to trade-off between latency (via the statless serving function pattern) and throughput (via batch serving pattern)\n",
    "\n",
    "- Typically, a Lambda architecture is supported by having separate systems for online serving and batch serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 18: Continued Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
