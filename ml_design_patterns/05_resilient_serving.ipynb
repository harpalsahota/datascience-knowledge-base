{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desing Patterns for Resilient Serving\n",
    "\n",
    "- Software deployed into production environments is expected to be resilient and require little in the way of human intervention to keep it running\n",
    "- Stateless serving\n",
    "    - This patter allows to scale and handle thousands or even millions of predictions requests per second\n",
    "- Batch Serving\n",
    "    - Asynchronously handle occasional or periodic requests for millions to billions of predictions\n",
    "    \n",
    "- Continued model evaluation\n",
    "    - Patterns that handle common problem of detecting when a deployed model is no longer fit-for-purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 16: Stateless Serving Function\n",
    "\n",
    "- A statless function is a function whose outputs are determined purely by its inputs\n",
    "- A function that maintains a counter of the number of times it's been called is stateful\n",
    "    - Can be done via class variables\n",
    "    \n",
    "- Because stateless components don't have any state, they can be shared by multiple clients\n",
    "- Servers typically create a pool of statelss components and use them to service client requests as they come in\n",
    "- On the other hand, stateful components will need to represent each clients conversational state\n",
    "- Lifecycle of stateless components needs to be mandages by the serve\n",
    "    - e.g. initialised on clients first request and then destroyed when the client terminates or times out\n",
    "    - Becuase of these factors stateless components are highly scalable\n",
    "    - Where as stateful components are expensive and difficult to manage\n",
    "- Web applications transfer state from the clients to the server with each call\n",
    "\n",
    "- In ML a lot of state is captured during training\n",
    "- When the model is exported / saved we reqiuire the model framework to keep track of these stateful variables\n",
    "\n",
    "- When stateless functions are used, it simplifies the server code and makes it more scalable but can make client code more complicated\n",
    "    - e.g. A spelling correction model has to know the previous few words in order to correct words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "- Image we have a deep learning model trained (kera flavour)\n",
    "- Model makes predictions on how positive the reviews are for a film, therefore has a embedding layer\n",
    "- We use this model to make predictions via the `predict` call\n",
    "\n",
    "- There are several problems carrying out inference by calling `model.predict()` on an in memory object \n",
    "    - Loaded enitre keras model into memory, with all embeddings and layers this can be very large for deep learning models\n",
    "    - The preceding architechture imposes limits on the latency that can be achieved because calls to the `predict()` method have to be sent one by one\n",
    "    - Model input and output is most effective for training and may not be user friendly. The model may output [logits](https://en.wikipedia.org/wiki/Logit) but clients may want the sigmoid of that so that the output range is between 0 and 1. This can then me interpreted more as a probability. Model may have also been trained on compressed binary records where as the input format from the client may be a JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "1. Export the model into a format that captures the mathmatical core of the model and is programming language agnostic\n",
    "2. In the production system, the formula consisting of the \"forward\" calculations of the model is resorted as a stateless function\n",
    "3. The stateless function is deployed into a framework that provided a REST endpoint\n",
    "\n",
    "#### Model Export\n",
    "\n",
    "- Can use [ONNX](https://onnx.ai)\n",
    "- Learning rate, dropout etc... don't need to be saved. This is what ONNX does.\n",
    "\n",
    "#### Inference in Python\n",
    "\n",
    "- In production the model's formula is restored along with any other files as a stateless function\n",
    "- The function must conform to a specidic model signature with the input and output variable names and input types\n",
    "- The signature specifies what the input and output of the model is \n",
    "\n",
    "#### Create Webendpoint\n",
    "\n",
    "- Should define the serving function as a global variable or a singleton class so that it isn't reloaded in response to every request.\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "#### AutoScaling\n",
    "\n",
    "- Scaling to milltions of requests per second is a well understood engineering problem\n",
    "- Rather than building services unique to ML we can rely on decades of engineering work that has gone into building resilient web applications\n",
    "- Modern cloud services offer auto-scaling services\n",
    "- Some ML framworks have their own serving subsystem:\n",
    "    - PyTorch: TorchServe\n",
    "    - TensorFLow: TensorFlow Serving\n",
    "    \n",
    "#### Fully Managed\n",
    "\n",
    "- Some cloud providers also have the capability of serving models e.g. sagemaker\n",
    "\n",
    "### Trade-Offs and Alternatives\n",
    "\n",
    "#### Customer Serving Function\n",
    "\n",
    "- Use a custom function to return the desired result to the client\n",
    "\n",
    "#### Multiple Signatures\n",
    "\n",
    "- There can be inexpensive (e.g. sigmoid) and expensive functions to run at inference time\n",
    "- It would not be good to return both to the client with each request\n",
    "- The client must explicitily request the inference from the expensive function\n",
    "\n",
    "#### Prediction Library\n",
    "\n",
    "- Instead of deploying the serving function as a microservice that can be called via a REST API, implement the prediction code as a library function\n",
    "- The library would load the model the first time it is called\n",
    "- Developers who need to predict with the libarary can then include the library with their application\n",
    "\n",
    "- A library function is better alternative than a microservice if the model cannot be called over a netowrk for physical or performance reasons\n",
    "- The libary function also places the computational burden on the clience and this might be preferable from a budgetary standpoint\n",
    "\n",
    "- Draw back is that maintenance and updates of the model are difficult\n",
    "- All client code that uses the model will have to be updated to use the new version of the library\n",
    "- The more the model is updated the more attractive the microservice approach becomes\n",
    "- The libaray approach is also restircted to programming languages for which the libraries are written\n",
    "- Where as REST API opens up the model to applications written in any modern language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Pattern 17: Batch Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
